\documentclass[a4paper, 12pt]{article}

\input{preamble.tex}
\renewcommand{\abstractname}{Аннотация}

\title{Автоматическое выделение терминов для тематического моделирования}

\author{
	  Никитина Мария Александровна\\
	\texttt{nikitina.mariia@phystech.edu} \\
 \\
        \textbf{Консультант: Потапова Полина Сергеевна} \\
	\texttt{potapov.polina@gmail.com} \\
 \\
        \textbf{Эксперт: Доктор ф-м наук, Воронцов Константин Вячеславович} \\
}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
В данной статье рассматривается задача автоматического выделения терминов в коллекции документов. Новые научные термины появляются каждый день.
Ручное извлечение терминов с привлечением узукоспециализированных
специалистов является трудозатратным. Цель настоящей работы --- обнаружение таких терминов в коллекциях документов в автоматическом режиме. Для решения данной задачи используется метод выделения коллокаций (TopMine) в сочетании с модульной технологией тематического моделирования (с использованием библиотеки BigARTM) и современные методы, основанные на нейросетевых моделях языка. Производится сравнение рассматриваемых решений.
\end{abstract}

\keywords{тематическое моделирование \and TopMine \and BigARTM \and Automatic Term Extraction}

\section{Введение}

        Поиск научных терминов в коллекции документов вручную практически невозможен из-за слишком больших объёмов работы. Для экономии ресурсов и времени предлагается рассмотреть задачу автоматического выделения терминов. К её решению можно подойти с разных сторон. Например, использовать сочетание метода выделения коллокаций с технологией математического моделирования \citep{ElKishky2014}. \textit{Коллокация} -- слово или словосочетание, имеющее признаки синтаксически и семантически целостной единицы

        \textit{Тематическое моделирование} -- это технология обработки естественного языка, направленная на определение тем, к которым относится текстовый документ из коллекции, и какие слова каждую тему образуют. Иначе говоря, тематическая модель осуществляет \textit{мягкую кластеризацию}, выбирая для документа кластеры-темы.

        \textit{Вероятностная тематическая модель} определяет вероятности тем в каждом документе и вероятности слов в каждой теме. Большим отличием такой модели от глубоких нейронных сетей типа BERT \citep{bert} или GPT-3 \citep{Brown2020LanguageMA} является простота организации и свойство интерпретируемости в ущерб качеству предсказания вероятности появления слов в документе. Векторное представление тяжёлой нейросети всё ещё не удалось интерпретировать, в то время как тематический эмбединг -- это вектор вероятностей тем.

        Новизной данной статьи является сравнение этих двух подходов. Нейросеть использована готовая. 
        % Здесь нужно вставить, какую нейросеть используем.
        Для построения же тематической модели требуется подбор \textit{регуляризаторов} -- критериев, учитывающих специфические особенности данных или предметной области, от подбора которых значительно зависит качество определения основных тем документов. В данной работе используется модель \textit{аддитивной регуляризации тематической модели, ARTM} \citep{vorontsov2020}. Для построения тематической модели с аддитивной регуляризацией используется библиотека BigARTM \citep{Vorontsov2015} с открытым кодом.

        Перед выполнением кластеризации необходимо выделить из коллекции документов ключевые слова и словосочетания и отбросить те, что не несут основной смысловой нагрузки. Поиск составных терминов является нетривиальной и трудоёмкой задачей. Для её решения используется метод поиска коллокаций TopMine, использующий информацию о частоте и совстречаемости слов в коллекции \citep{shatalov2019}.

        % Ещё какая-то информация про датасеты

        С учётом интерпретируемости и простоты тематическая модель является хорошей заменой нейросети. Предшествующие исследования предлагаемого подхода показали хорошие результаты как по полноте, так и по вычислительной эффективности. В работе сравниваются тематическая модель и сложная нейросетевая модель, анализируется их качество для рассматриваемой задачи.

\section{Постановка задачи}

        Основная задача -- построение модели ATE (Automatic Term Extraction) для автоматического выделения словосочетаний, являющихся терминами предметной области, в текстах научных статей. Предлагается использовать эффективные методы выделения коллокаций и тематические модели для определения «тематичности» словосочетания. Модель должна обучаться без учителя.

        Для решения поставленной задачи применяются алгоритмы поиска коллокаций TopMine с последующей фильтрацией по критерию тематичности, подбор гиперпараметров тематической модели и критерия тематичности.

        Задача называется корректно поставленной по Адамару, если её решение существует, единственно и устойчиво. В общем случае построение тематической модели – некорректно поставленная задача по Адамару, поэтому её нужно дополнить регуляризаторами. В практических задачах автоматической обработки текстов существует очень много критериев и ограничений.

        Пусть $p_{\omega d}$ -- вероятность появления терма $\omega$ в документе $d$, $\phi_{\omega t}$ -- вероятность того, что терм $\omega$ относится к теме $t$, $\theta_{td}$ -- вероятность встречи темы $t$ в документе $d$. Тогда $P = (p_{\omega d})_{W \times D}$ -- матрица частот термов в документах, $\Phi = (\phi_{\omega t})_{W \times T}$ -- матрица термов тем, $\Theta = (\theta_{td})_{T \times D}$ -- матрица тем документов. $W$, $D$, $T$ -- множества всех термов, документов и тем соответственно.
       
        Аддитивная регуляризация тематических моделей основана на максимизации логарифма правдоподобия и регуляризаторов $R_i(\Phi, \Theta)$ с неотрицательными коэффициентами регуляризации $\tau_i$, $i = 1, ..., k$ \citep{vorontsov2020}:
        \begin{equation}
            \sum\limits_{d \in D}\sum\limits_{w \in d}\ln\sum\limits_{t \in T}\phi_{\omega t}\theta_{td} + R(\Phi, \Theta) \to \max\limits_{\Phi, \Theta}; ~~~~~ R(\Phi, \Theta) = \sum\limits_{i = 1}^k\tau_iR_i(\Phi, \Theta);
        \end{equation}
        при ограничениях неотрицательности и нормировки:
        \begin{equation}
            \sum\limits_{w \in W}\phi_{\omega t} = 1; ~~~ \phi_{\omega t} \geq 0; ~~~~~ \sum\limits_{t \in T}\theta_{td} = 1; ~~~ \theta_{td} \geq 0.
        \end{equation}

    Решается задача нахождения разложения $P = \Phi\Theta$ при достижении максимума. Для её решения применяется EM-алгоритм. Таким образом, основной проблемой построения тематической модели становится поиск регуляризаторов $R_i(\Phi, \Theta)$, подходящих под нашу задачу поиска терминов в коллекции документов.

\section{Вычислительный эксперимент}

    Для обучения модели используется открытый датасет ACL RD-TEC \citep{QZadeh2014}, в котором собраны статьи на английском языке с 1965 по 2006 год из области компьютерной лингвистики. Его описание представлено в таблице \ref{table:Dataset}. Для проведения эксперимента из него удаляются документы, содержащие менее 20 терминов. В результате получается датасет из 9,095 статей. Распределение терминов по документам представлено на графике \ref{fg:Dataset}.

 \begin{table}[t]%\small
    \caption{Описание датасета ACL RD-TEC}
    \label{table:Dataset}
    \centering\medskip%\tabcolsep=2pt%\small
    \begin{tabular}{| p{75 pt} | p{50 pt} | p{70 pt} | p{70 pt} | p{70 pt} |}
    \hline
        Датасет
            & Год
            & Количество документов
            & Количество слов
            & Количество терминов \\ \hline
        ACL RD-TEC
            & 2014
            & 10,922
            & 36,729,513
            & 82,000 \\ \hline
    \hline
    \end{tabular}
\end{table}

\begin{figure}[!ht]
    {\includegraphics[scale = 1.2]{Pictures/Statistics.eps}}
    \caption{Распределение терминов по документам}
    \label{fg:Dataset}
\end{figure}


\section{Анализ ошибки}

\section{Заключение}

\bibliographystyle{plain}
\bibliography{references.bib}

\end{document}
