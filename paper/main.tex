\documentclass[a4paper, 12pt]{article}

\input{preamble.tex}
\renewcommand{\abstractname}{Аннотация}

\title{Автоматическое выделение терминов для тематического моделирования}

\author{
	  Никитина Мария \\
	\texttt{nikitina.mariia@phystech.edu} \\
}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
В настоящее время каждый день появляются новые научные термины. Необходимо научиться находить их в коллекции документов. Делать это вручную долго и дорого, потому что нужно привлекать узкоспециализированных специалистов. В данной статье рассматриваются два решения этой проблемы: метод выделения коллокаций (TopMine) в сочетании с модульной технологией тематического моделирования (с использованием библиотеки BigARTM) и современные методы, основанные на нейросетевых моделях языка. Эти два решения ранее не сравнивались.
\end{abstract}

\keywords{тематическое моделирование \and TopMine \and BigARTM \and выделение коллокаций}

\section{Введение}

        \textit{Тематическое моделирование} -- это технология обработки естественного языка, направленная на определение тем, к которым относится текстовый документ из коллекции, и какие слова каждую тему образуют. Иначе говоря, тематическая модель осуществляет \textit{мягкую кластеризацию}, выбирая для документа кластеры-темы.

        \textit{Вероятностная тематическая модель} определяет вероятности тем в каждом документе и вероятности слов в каждой теме. Большим отличием такой модели от глубоких нейронных сетей типа BERT \citep{bert} или GPT-3 \citep{Brown2020LanguageMA} является простота организации и свойство интерпретируемости в ущерб качеству предсказания вероятности появления слов в документе. Векторное представление тяжёлой нейросети всё ещё не удалось интерпретировать, в то время как тематический эмбединг -- это вектор вероятностей тем.

        Основной задачей и новизной данной статьи является сравнение этих двух подходов. Нейросеть использована готовая. 
        % Здесь нужно вставить, какую нейросеть используем.
        Для построения же тематической модели требуется составление \textit{регуляризаторов} -- критериев, учитывающих специфические особенности данных или предметной области, от подбора которых значительно зависит качество определения основных тем документов (\textit{Аддитивная регуляризация тематических моделей, ARTM}). К основному алгоритму регуляризаторы подключаются как модули с помощью библиотеки BigARTM \citep{Vorontsov2015} с открытым кодом.

        Перед выполнением кластеризации необходимо выделить из коллекции документов ключевые слова и словосочетания и отбросить те, что не несут основной смысловой нагрузки. Поиск составных терминов является нетривиальной и трудоёмкой задачей. Для её решения используется метод поиска коллокаций TopMine, использующий информацию о частоте и совстречаемости слов в коллекции \citep{shatalov2019}.

        % Ещё какая-то информация про датасеты

        С учётом интерпретируемости и простоты тематическая модель может стать хорошей заменой нейросети. Предшествующие исследования предлагаемого подхода показали хорошие результаты как по полноте, так и по вычислительной эффективности. Однако они до сих пор не сравнивались с нейросетевыми моделями. Важно понять, насколько хорошо тематическая модель выполняет рассчитанную под неё задачу по сравнению со сложной нейронной сетью.

\section{Постановка задачи}
        Задача называется \textit{корректно поставленной} по Адамару, если её решение существует, единственно и устойчиво. В общем случае построение тематической модели -- некорректно поставленная задача, её нужно дополнить регуляризаторами. В практических задачах автоматической обработки текстов существует очень много критериев и ограничений.

        Пусть $p_{\omega d}$ -- вероятность появления терма $\omega$ в документе $d$, $\phi_{\omega t}$ -- вероятность того, что терм $\omega$ относится к теме $t$, $\theta_{td}$ -- вероятность встречи темы $t$ в документе $d$. Тогда $P = (p_{\omega d})_{W \times D}$ -- матрица частот термов в документах, $\Phi = (\phi_{\omega t})_{W \times T}$ -- матрица термов тем, $\Theta = (\theta_{td})_{T \times D}$ -- матрица тем документов. $W$, $D$, $T$ -- множества всех термов, документов и тем соответственно.
       
        Аддитивная регуляризация тематических моделей основана на максимизации логарифма правдоподобия и регуляризаторов $R_i(\Phi, \Theta)$ с неотрицательными коэффициентами регуляризации $\tau_i$, $i = 1, ..., k$ \citep{vorontsov2020}:
        \begin{equation}
            \sum\limits_{d \in D}\sum\limits_{w \in d}\ln\sum\limits_{t \in T}\phi_{\omega t}\theta_{td} + R(\Phi, \Theta) \to \max\limits_{\Phi, \Theta}; ~~~~~ R(\Phi, \Theta) = \sum\limits_{i = 1}^k\tau_iR_i(\Phi, \Theta);
        \end{equation}
        при ограничениях неотрицательности и нормировки:
        \begin{equation}
            \sum\limits_{w \in W}\phi_{\omega t} = 1; ~~~ \phi_{\omega t} \geq 0; ~~~~~ \sum\limits_{t \in T}\theta_{td} = 1; ~~~ \theta_{td} \geq 0.
        \end{equation}

        Решается задача нахождения разложения $P = \Phi\Theta$ при достижении максимума. Для её решения применяется EM-алгоритм. Таким образом, основной проблемой становится поиск регуляризаторов $R_i(\Phi, \Theta)$, подходящих под нашу задачу поиска терминов в коллекции документов.

\section{Вычислительный эксперимент}

\section{Анализ ошибки}

\section{Заключение}

\bibliographystyle{plain}
\bibliography{Nikitina2023.bib}

\end{document}
