{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bigartm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use the BigARTM, you need to have a python version 3.8 or lower\n",
    "import artm\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import re\n",
    "from pathlib import *\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking library\n",
    "artm.ARTM(num_topics=1)\n",
    "print(artm.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words from the text and deleting documents in which there are less than 5 terms from one word. For a basic experiment, you can immediately upload a file Train_1.vw.txt from the disk. There is a link to it in the folder \"code\" in the file \"ACL_RD_TEC.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation(input_path = 'Dataset/Train_Test/Train/Train_1_stem.vw.txt', \\\n",
    "                output_path = 'Dataset/Train_Test/Train/Train_1.vw.txt'):\n",
    "    \"\"\"\n",
    "    Removing stop words from the text and deleting documents in which there are less than 5 terms\n",
    "            Parameters:\n",
    "                    input_path (str): directory with documents sorted by year to load\n",
    "                    output_path (str): directory with documents sorted by year to save\n",
    "    \"\"\"\n",
    "\n",
    "counter = 0\n",
    "with open(input_path, 'r', errors=\"ignore\") as fin:\n",
    "    with open(output_path, 'w') as fout:\n",
    "        for line in fin.readlines():\n",
    "            words = line.split()\n",
    "            if len(words) < 5:\n",
    "                continue\n",
    "            else:\n",
    "                words[0] = 'doc_' + str(counter)\n",
    "                counter += 1\n",
    "                clean = ' '.join([word for word in words if (len(word) > 2 and not word in all_stopwords_gensim)])\n",
    "                fout.write(clean + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to highlight subject and background topics. Here the first 2 topics are background, followed by 149 subject topics. The dictionary is created by a function from the BigARTM library. The dictionary is created by a function from the BigARTM library. It also breaks the dataset into butches. One file is submitted to the input. It contains a dataset in the Vowpal Wabbit format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling a dictionary and divide the dataset into butches\n",
    "vw_filaname = 'terminology-extraction-master/ACTER/en/texts.vw.txt'\n",
    "bv = artm.BatchVectorizer(data_path=vw_filaname, data_format='vowpal_wabbit', batch_size=900, target_folder='batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(bv, num_topics=150, tau_dec=0.1, tau_dec_add=0.025, tau_phi=-0.1, tau_phi_add=0.2, \\\n",
    "              tau_theta=-0.1, tau_theta_add=0.2):\n",
    "    \"\"\"\n",
    "    Creating a model and adding regularizers to it\n",
    "            Parameters:\n",
    "                    bv (artm.BatchVectorizer): needed to set dictionary\n",
    "                    num_topics (int): number of hidden topics -- number of columns of the phi matrix\n",
    "                    tau_dec (float): the value of the decorrelation coefficient for subject topics\n",
    "                    tau_dec_add (float): the value of the decorrelation coefficient for background topics\n",
    "                    tau_phi (float): the value of the smooth-sparce coefficient of subject topics of phi-matrix\n",
    "                    tau_phi_add (float): the value of the smooth-sparce coefficient of background topics of phi-matrix\n",
    "                    tau_theta (float): the value of the smooth-sparce coefficient of subject topics of theta-matrix\n",
    "                    tau_theta_add (float): the value of the smooth-sparce coefficient of background topics of theta-matrix\n",
    "            Return:\n",
    "                    model (artm.ARTM): the ARTM model\n",
    "    \"\"\"\n",
    "    # list of subject topics\n",
    "    list_of_topics = []\n",
    "    for i in range(2, num_topics + 1):\n",
    "        list_of_topics.append('topic_' + str(i))\n",
    "    \n",
    "    # model\n",
    "    model = artm.ARTM(num_topics=num_topics, dictionary=bv.dictionary)\n",
    "    model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=bv.dictionary))\n",
    "    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))\n",
    "    \n",
    "    # regularizers\n",
    "    reg = artm.DecorrelatorPhiRegularizer(name='decor', tau = tau_dec, topic_names=list_of_topics)\n",
    "    reg_add = artm.DecorrelatorPhiRegularizer(name='decor_add', tau = tau_dec_add, topic_names=['topic_0', 'topic_1'])\n",
    "    reg_phi = artm.SmoothSparsePhiRegularizer(name='ssphi', tau=tau_phi, topic_names=list_of_topics)\n",
    "    reg_phi_add = artm.SmoothSparsePhiRegularizer(name='ssphi_add', tau=tau_phi_add, topic_names=['topic_0', 'topic_1'])\n",
    "    reg_theta = artm.SmoothSparseThetaRegularizer(name='sstheta', tau=tau_theta, topic_names=list_of_topics)\n",
    "    reg_theta_add = artm.SmoothSparseThetaRegularizer(name='sstheta_add', tau=tau_theta_add, topic_names=['topic_0', 'topic_1'])\n",
    "    model.regularizers.add(reg)\n",
    "    model.regularizers.add(reg_add)\n",
    "    model.regularizers.add(reg_phi)\n",
    "    model.regularizers.add(reg_phi_add)\n",
    "    model.regularizers.add(reg_theta)\n",
    "    model.regularizers.add(reg_theta_add)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = set_model(bv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexy is one of the quality metrics. The smaller it is, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for i in range(15):\n",
    "    model.fit_offline(bv, num_collection_passes=1)\n",
    "    print(f'Iter #{i}, perplexity: {model.score_tracker[\"perplexity\"].last_value}')\n",
    "    print(f'sparse: {model.score_tracker[\"sparse\"].last_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 terms with the highest probability in each topic\n",
    "top_tokens = model.score_tracker['top-tokens'].last_tokens\n",
    "\n",
    "for topic_name in model.topic_names:\n",
    "    print(top_tokens[topic_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the phi matrix in 'csv' format\n",
    "name = 'Phi.csv'\n",
    "model.get_phi(topic_names=model.topic_names, class_ids=['@default_class'], model_name = model.model_pwt).to_csv(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_terms(phi_name='Phi.csv', num_topics=150, min_prob=0.03, max_topics=3):\n",
    "    \"\"\"\n",
    "    Search for words that have a high probability in a small number of topics\n",
    "            Parameters:\n",
    "                    phi_name (str): name of the file in the 'csv' format with the phi-matrix\n",
    "                    num_topics (int): number of hidden topics -- number of columns of the phi-matrix\n",
    "                    min_prob (float): minimum probability for the term\n",
    "                    max_topics (int): maximum number of high probability topics for a term\n",
    "                    \n",
    "            Return:\n",
    "                    terms (list): the list of all possible terms in the dataset\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "\n",
    "    with open(phi_name) as f:\n",
    "        reader = csv.reader(f)\n",
    "        f = 0\n",
    "        for row in reader:\n",
    "            if f == 0:\n",
    "                f = 1\n",
    "                continue\n",
    "            else:\n",
    "                count = 0\n",
    "                for i in range(1, num_topics + 1):\n",
    "                    if float(row[i]) > min_prob:\n",
    "                        count += 1\n",
    "                if 0 < count <= max_topics:\n",
    "                    terms.append(re.sub(r'\\'|\\)', '', row[0].split(', ')[1]))\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = find_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(terms, truth_json='Dataset/Train_test/Train_1.json', input_dir = 'Dataset/TopMine_texts/'):\n",
    "    \"\"\"\n",
    "    Search for words that have a high probability in a small number of topics\n",
    "            Parameters:\n",
    "                    terms (list): the list of all possible terms in the dataset\n",
    "                    truth_json (str): the name of the file with markup in 'json' format\n",
    "                    input_dir (str): directory with texts after TopMine\n",
    "            Return:\n",
    "                    precision (list): precision for each document\n",
    "                    recall (list): recall for each document\n",
    "    \"\"\"\n",
    "    precision = []\n",
    "    recall = []\n",
    "    iteration = 0\n",
    "    terms = set(terms)\n",
    "    with open(truth_json, 'r') as fin:\n",
    "        truth = json.load(fin)\n",
    "\n",
    "    for k in truth.keys():\n",
    "        # tp -- true-positive, fp -- false-positive, fn -- false-negative\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        \n",
    "        # markup stemming and deliting numbers\n",
    "        truth_str = ' '.join(truth[k])\n",
    "        truth_str = re.sub(r\"\\d+\", \"\", truth_str, flags=re.UNICODE)\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "        sent = TextBlob(truth_str)\n",
    "        gr_truth = set([stemmer.stem(w) for w in sent.words if len(w) > 2])\n",
    "        \n",
    "        fn = len(gr_truth)\n",
    "        \n",
    "        input_path = input_dir + k + '.txt'\n",
    "        with open(input_path, 'r') as fin:\n",
    "            result = []\n",
    "            words = set(fin.read().split())\n",
    "            result = words & terms\n",
    "\n",
    "        # progress\n",
    "        iteration += 1\n",
    "        if iteration % 1000 == 0:\n",
    "            print('Progress: ', iteration)\n",
    "\n",
    "        # counting results\n",
    "        match = result & gr_truth\n",
    "        tp = len(match)\n",
    "        fn = fn - tp\n",
    "        fp = len(result) - tp\n",
    "        if (tp + fp) >= 1:\n",
    "            precision.append(tp / (tp + fp))\n",
    "        else:\n",
    "            precision.append(0)\n",
    "        if (tp + fn) >= 1:\n",
    "            recall.append(tp / (tp + fn))\n",
    "        else:\n",
    "            recall.append(0)\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = results(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = sum(precision) / len(precision)\n",
    "print('Precision:', prec)\n",
    "\n",
    "rec = sum(recall) / len(recall)\n",
    "print('Recall:', rec)\n",
    "\n",
    "f1 = 2 * prec * rec / (prec + rec)\n",
    "print('F1:', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
